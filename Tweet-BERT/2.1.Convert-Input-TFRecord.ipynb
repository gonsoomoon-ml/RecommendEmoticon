{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Module 2.0] Tweet Text를 BERT Embedding Vector로 변환\n",
    "이 노트북에서는 아래와 같은 작업이 진행 됩니다. 참고로 노트북의 코드는 아래 Reference 코드를 거의 사용 했습니다.\n",
    "\n",
    "- Preprocess Script 생성\n",
    "    - Input Text --> BERT Feature Vector 로 변환 --> TF Record로 변환\n",
    "- 위 스크립트를 테스트 목적으로 Local Notebook에서 실행\n",
    "- SageMaker Processor Job으로 2개의 인스턴스를 사용하여 위 스크립트를 실행\n",
    "    - Train, Validation, Test의 각각 2개의 TF Records 파일이 S3에 저장됨.\n",
    "    \n",
    "---\n",
    "이 노트북은 약 6분 정도 소요 됩니다.    \n",
    "\n",
    "---\n",
    "Reference:\n",
    "    - https://github.com/data-science-on-aws/workshop/blob/master/06_prepare/02_Prepare_Dataset_ProcessingJob_BERT_Scikit.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from time import gmtime, strftime\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Preprocess Script\n",
    "이 파일은 아래 소스에서 Label의 5개 --> 10개 변경한 버전 임.\n",
    "- 소스: \n",
    "    - https://github.com/data-science-on-aws/workshop/blob/master/06_prepare/preprocess-scikit-text-to-bert.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess-scikit-text-to-bert.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess-scikit-text-to-bert.py\n",
    "\n",
    "# 이 파일은 아래 소스에서 Label의 5개 --> 10개 변경한 버전 임.\n",
    "# https://github.com/data-science-on-aws/workshop/blob/master/06_prepare/preprocess-scikit-text-to-bert.py\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "import functools\n",
    "import multiprocessing\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'tensorflow==2.1.0'])\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'transformers==2.8.0'])\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import re\n",
    "import collections\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "DATA_COLUMN = 'TWEET'\n",
    "LABEL_COLUMN = 'LABEL'\n",
    "LABEL_VALUES = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    \n",
    "label_map = {}\n",
    "for (i, label) in enumerate(LABEL_VALUES):\n",
    "    label_map[label] = i\n",
    "\n",
    "class InputFeatures(object):\n",
    "  \"\"\"BERT feature vectors.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               input_ids,\n",
    "               input_mask,\n",
    "               segment_ids,\n",
    "               label_id):\n",
    "    self.input_ids = input_ids\n",
    "    self.input_mask = input_mask\n",
    "    self.segment_ids = segment_ids\n",
    "    self.label_id = label_id\n",
    "    \n",
    "    \n",
    "class Input(object):\n",
    "  \"\"\"A single training/test input for sequence classification.\"\"\"\n",
    "\n",
    "  def __init__(self, text, label=None):\n",
    "    \"\"\"Constructs an Input.\n",
    "    Args:\n",
    "      text: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "    self.text = text\n",
    "    self.label = label\n",
    "\n",
    "def convert_input(text_input, max_seq_length):\n",
    "    # First, we need to preprocess our data so that it matches the data BERT was trained on:\n",
    "    #\n",
    "    # 1. Lowercase our text (if we're using a BERT lowercase model)\n",
    "    # 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
    "    # 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
    "    # \n",
    "    # Fortunately, the Transformers tokenizer does this for us!\n",
    "    #\n",
    "    tokens = tokenizer.tokenize(text_input.text)    \n",
    "\n",
    "    encode_plus_tokens = tokenizer.encode_plus(text_input.text,\n",
    "                                               pad_to_max_length=True,\n",
    "                                               max_length=max_seq_length)\n",
    "\n",
    "    # Convert the text-based tokens to ids from the pre-trained BERT vocabulary\n",
    "    input_ids = encode_plus_tokens['input_ids']\n",
    "    # Specifies which tokens BERT should pay attention to (0 or 1)\n",
    "    input_mask = encode_plus_tokens['attention_mask']\n",
    "    # Segment Ids are always 0 for single-sequence tasks (or 1 if two-sequence tasks)\n",
    "    segment_ids = [0] * max_seq_length\n",
    "\n",
    "    # Label for our training data (star_rating 1 through 5)\n",
    "    label_id = label_map[text_input.label]\n",
    "\n",
    "    features = InputFeatures(\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids,\n",
    "        label_id=label_id)\n",
    "\n",
    "    return features\n",
    "\n",
    "def convert_features_to_tfrecord(inputs,\n",
    "                                 output_file,\n",
    "                                 max_seq_length):\n",
    "    \"\"\"Convert a set of `Input`s to a TFRecord file.\"\"\"\n",
    "\n",
    "    tfrecord_writer = tf.io.TFRecordWriter(output_file)\n",
    "\n",
    "    for (input_idx, text_input) in enumerate(inputs):\n",
    "\n",
    "        bert_features = convert_input(text_input, max_seq_length)\n",
    "\n",
    "        tfrecord_features = collections.OrderedDict()\n",
    "\n",
    "        tfrecord_features['input_ids'] = tf.train.Feature(int64_list=tf.train.Int64List(value=bert_features.input_ids))\n",
    "        tfrecord_features['input_mask'] = tf.train.Feature(int64_list=tf.train.Int64List(value=bert_features.input_mask))\n",
    "        tfrecord_features['segment_ids'] = tf.train.Feature(int64_list=tf.train.Int64List(value=bert_features.segment_ids))\n",
    "        tfrecord_features['label_ids'] = tf.train.Feature(int64_list=tf.train.Int64List(value=[bert_features.label_id]))\n",
    "\n",
    "        tfrecord = tf.train.Example(features=tf.train.Features(feature=tfrecord_features))\n",
    "\n",
    "        tfrecord_writer.write(tfrecord.SerializeToString())\n",
    "\n",
    "    tfrecord_writer.close()\n",
    "\n",
    "    \n",
    "    \n",
    "def list_arg(raw_value):\n",
    "    \"\"\"argparse type for a list of strings\"\"\"\n",
    "    return str(raw_value).split(',')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def _transform_tsv_to_tfrecord(file, \n",
    "                               max_seq_length, \n",
    "                               balance_dataset):\n",
    "    print('file {}'.format(file))\n",
    "    print('max_seq_length {}'.format(max_seq_length))\n",
    "    print('balance_dataset {}'.format(balance_dataset))\n",
    "\n",
    "    filename_without_extension = Path(Path(file).stem).stem\n",
    "\n",
    "    df = pd.read_csv(file, \n",
    "                     compression='gzip')\n",
    "\n",
    "    df.isna().values.any()\n",
    "    df = df.dropna()\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    print('Shape of dataframe {}'.format(df.shape))\n",
    "\n",
    "        \n",
    "    print('Shape of dataframe before splitting {}'.format(df.shape))\n",
    "    \n",
    "    print('train split percentage {}'.format(args.train_split_percentage))\n",
    "    print('validation split percentage {}'.format(args.validation_split_percentage))\n",
    "    print('test split percentage {}'.format(args.test_split_percentage))    \n",
    "    \n",
    "    holdout_percentage = 1.00 - args.train_split_percentage\n",
    "    print('holdout percentage {}'.format(holdout_percentage))\n",
    "    df_train, df_holdout = train_test_split(df, \n",
    "                                            test_size=holdout_percentage, \n",
    "                                            stratify=df[LABEL_COLUMN])\n",
    "\n",
    "    test_holdout_percentage = args.test_split_percentage / holdout_percentage\n",
    "    print('test holdout percentage {}'.format(test_holdout_percentage))\n",
    "    df_validation, df_test = train_test_split(df_holdout, \n",
    "                                              test_size=test_holdout_percentage,\n",
    "                                              stratify=df_holdout[LABEL_COLUMN])\n",
    "    \n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "    df_validation = df_validation.reset_index(drop=True)\n",
    "    df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "    print('Shape of train dataframe {}'.format(df_train.shape))\n",
    "    print('Shape of validation dataframe {}'.format(df_validation.shape))\n",
    "    print('Shape of test dataframe {}'.format(df_test.shape))\n",
    "\n",
    "    train_inputs = df_train.apply(lambda x: Input(text = x[DATA_COLUMN], \n",
    "                                                         label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "    validation_inputs = df_validation.apply(lambda x: Input(text = x[DATA_COLUMN], \n",
    "                                                            label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "    test_inputs = df_test.apply(lambda x: Input(text = x[DATA_COLUMN], \n",
    "                                                label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "    # Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n",
    "    # \n",
    "    # \n",
    "    # 1. Lowercase our text (if we're using a BERT lowercase model)\n",
    "    # 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
    "    # 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
    "    # 4. Map our words to indexes using a vocab file that BERT provides\n",
    "    # 5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n",
    "    # 6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
    "    # \n",
    "    # We don't have to worry about these details.  The Transformers tokenizer does this for us.\n",
    "    # \n",
    "    train_data = '{}/bert/train'.format(args.output_data)\n",
    "    validation_data = '{}/bert/validation'.format(args.output_data)\n",
    "    test_data = '{}/bert/test'.format(args.output_data)\n",
    "\n",
    "    # Convert our train and validation features to InputFeatures (.tfrecord protobuf) that works with BERT and TensorFlow.\n",
    "    df_train_embeddings = convert_features_to_tfrecord(train_inputs, \n",
    "                                                       '{}/part-{}-{}.tfrecord'.format(train_data, args.current_host, filename_without_extension), \n",
    "                                                       max_seq_length)\n",
    "\n",
    "    df_validation_embeddings = convert_features_to_tfrecord(validation_inputs, '{}/part-{}-{}.tfrecord'.format(validation_data, args.current_host, filename_without_extension), max_seq_length)\n",
    "\n",
    "    df_test_embeddings = convert_features_to_tfrecord(test_inputs, '{}/part-{}-{}.tfrecord'.format(test_data, args.current_host, filename_without_extension), max_seq_length)\n",
    "        \n",
    "def parse_args():\n",
    "    # Unlike SageMaker training jobs (which have `SM_HOSTS` and `SM_CURRENT_HOST` env vars), processing jobs to need to parse the resource config file directly\n",
    "    resconfig = {}\n",
    "    try:\n",
    "        with open('/opt/ml/config/resourceconfig.json', 'r') as cfgfile:\n",
    "            resconfig = json.load(cfgfile)\n",
    "    except FileNotFoundError:\n",
    "        print('/opt/ml/config/resourceconfig.json not found.  current_host is unknown.')\n",
    "        pass # Ignore\n",
    "\n",
    "    # Local testing with CLI args\n",
    "    parser = argparse.ArgumentParser(description='Process')\n",
    "\n",
    "    parser.add_argument('--hosts', type=list_arg,\n",
    "        default=resconfig.get('hosts', ['unknown']),\n",
    "        help='Comma-separated list of host names running the job'\n",
    "    )\n",
    "    parser.add_argument('--current-host', type=str,\n",
    "        default=resconfig.get('current_host', 'unknown'),\n",
    "        help='Name of this host running the job'\n",
    "    )\n",
    "    parser.add_argument('--input-data', type=str,\n",
    "        default='/opt/ml/processing/input/data',\n",
    "    )\n",
    "    parser.add_argument('--output-data', type=str,\n",
    "        default='/opt/ml/processing/output',\n",
    "    )\n",
    "    parser.add_argument('--train-split-percentage', type=float,\n",
    "        default=0.90,\n",
    "    )\n",
    "    parser.add_argument('--validation-split-percentage', type=float,\n",
    "        default=0.05,\n",
    "    )    \n",
    "    parser.add_argument('--test-split-percentage', type=float,\n",
    "        default=0.05,\n",
    "    )\n",
    "    parser.add_argument('--balance-dataset', type=eval,\n",
    "        default=False\n",
    "    )\n",
    "    parser.add_argument('--max-seq-length', type=int,\n",
    "        default=32,\n",
    "    )  \n",
    "    \n",
    "    return parser.parse_args()\n",
    "        \n",
    "    \n",
    "def process(args):\n",
    "    print('Current host: {}'.format(args.current_host))\n",
    "    \n",
    "    train_data = None\n",
    "    validation_data = None\n",
    "    test_data = None\n",
    "\n",
    "    transform_tsv_to_tfrecord = functools.partial(_transform_tsv_to_tfrecord, \n",
    "                                                 max_seq_length=args.max_seq_length,\n",
    "                                                 balance_dataset=args.balance_dataset\n",
    "\n",
    "    )\n",
    "    input_files = glob.glob('{}/*'.format(args.input_data))\n",
    "    print(\"********** input files ***************\")    \n",
    "    print(\"args.input_data: \", args.input_data)\n",
    "    print(\"input_files: \", input_files)\n",
    "\n",
    "    num_cpus = multiprocessing.cpu_count()\n",
    "    print('num_cpus {}'.format(num_cpus))\n",
    "\n",
    "    p = multiprocessing.Pool(num_cpus)\n",
    "    p.map(transform_tsv_to_tfrecord, input_files)\n",
    "\n",
    "    ## 수정 부분\n",
    "    \n",
    "    print(\"********** Listing tf-record files ***************\")        \n",
    "    print('Listing contents of {}'.format(args.output_data))\n",
    "    train_tfrecord_path = '{}/bert/train'.format(args.output_data)\n",
    "    dirs_output = os.listdir(train_tfrecord_path)\n",
    "    for file in dirs_output:\n",
    "        print(file)\n",
    "\n",
    "    print('Listing contents of {}'.format(args.output_data))\n",
    "    validation_data_tfrecord_path = '{}/bert/validation'.format(args.output_data)\n",
    "    dirs_output = os.listdir(validation_data_tfrecord_path)\n",
    "    for file in dirs_output:\n",
    "        print(file)\n",
    "\n",
    "    print('Listing contents of {}'.format(args.output_data))\n",
    "    test_data_tfrecord_path = '{}/bert/test'.format(args.output_data)\n",
    "    dirs_output = os.listdir(test_data_tfrecord_path)\n",
    "    for file in dirs_output:\n",
    "        print(file)\n",
    "\n",
    "\n",
    "    print('Complete')\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    print('################ START #######################')    \n",
    "    print('Loaded arguments:')\n",
    "    print(args)\n",
    "    \n",
    "    print('Environment variables:')\n",
    "#     print(os.environ)\n",
    "\n",
    "    process(args)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Local Notebook\n",
    "아래는 로컬 노트북에서 ! python 으로 테스트를 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 데이터 폴더 위치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_split_data_dir = 'data/split'\n",
    "output_data_dir = 'data/output'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p data/output/bert/train\n",
    "! mkdir -p data/output/bert/validation\n",
    "! mkdir -p data/output/bert/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.1.0\n",
      "  Downloading tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8 MB)\n",
      "\u001b[K     |██████████████████████▍         | 295.1 MB 131.7 MB/s eta 0:00:01"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 421.8 MB 20 kB/s \n",
      "\u001b[?25hCollecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.36.1-cp36-cp36m-manylinux2014_x86_64.whl (4.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.1 MB 72.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-applications>=1.0.8\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 14.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (0.36.2)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 1.0 MB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.7.0\n",
      "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "\u001b[K     |████████████████████████████████| 129 kB 145.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
      "  Downloading tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n",
      "\u001b[K     |████████████████████████████████| 448 kB 102.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.19.5)\n",
      "Collecting gast==0.2.2\n",
      "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (0.2.0)\n",
      "Collecting keras-preprocessing>=1.1.0\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 2.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting astor>=0.6.0\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (3.15.2)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.12.1)\n",
      "Collecting scipy==1.4.1\n",
      "  Downloading scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.1 MB 81.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow==2.1.0) (1.15.0)\n",
      "Collecting tensorboard<2.2.0,>=2.1.0\n",
      "  Downloading tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 84.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras-applications>=1.0.8->tensorflow==2.1.0) (3.1.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.0.1)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.28.0-py2.py3-none-any.whl (136 kB)\n",
      "\u001b[K     |████████████████████████████████| 136 kB 130.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (49.6.0.post20210108)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 14.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.25.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.7.2)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.1-py3-none-any.whl (12 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 137.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.26.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.0.4)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 124.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cached-property in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from h5py->keras-applications>=1.0.8->tensorflow==2.1.0) (1.5.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.7.4.3)\n",
      "Building wheels for collected packages: gast, termcolor\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7538 sha256=c446abb756170687fd4da1abcd729015360de8b2d87de9fddf45169c116d1c62\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/19/a7/b9/0740c7a3a7d1d348f04823339274b90de25fbcd217b2ee1fbe\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=83146fb9fe730a72acf3ad988bb6c79927c05f6b66c318c6d8e854512387c1af\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "Successfully built gast termcolor\n",
      "Installing collected packages: pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, markdown, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-estimator, tensorboard, scipy, opt-einsum, keras-preprocessing, keras-applications, gast, astor, tensorflow\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.5.3\n",
      "    Uninstalling scipy-1.5.3:\n",
      "      Successfully uninstalled scipy-1.5.3\n",
      "Successfully installed absl-py-0.12.0 astor-0.8.1 cachetools-4.2.1 gast-0.2.2 google-auth-1.28.0 google-auth-oauthlib-0.4.4 grpcio-1.36.1 keras-applications-1.0.8 keras-preprocessing-1.1.2 markdown-3.3.4 oauthlib-3.1.0 opt-einsum-3.3.0 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 scipy-1.4.1 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0 termcolor-1.1.0\n",
      "2021-04-04 14:20:48.604455: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64:/usr/local/cuda-10.0/lib:/usr/local/cuda-10.0/efa/lib:/opt/amazon/efa/lib:/opt/amazon/efa/lib64:/usr/lib64/openmpi/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/mpi/lib:/lib/:/lib:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/opt/amazon/openmpi/lib64:/usr/local/cuda/lib:/opt/amazon/efa/lib64:/usr/local/mpi/lib:/usr/lib64/openmpi/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/mpi/lib:/lib/:\n",
      "2021-04-04 14:20:48.604576: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64:/usr/local/cuda-10.0/lib:/usr/local/cuda-10.0/efa/lib:/opt/amazon/efa/lib:/opt/amazon/efa/lib64:/usr/lib64/openmpi/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/mpi/lib:/lib/:/lib:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/opt/amazon/openmpi/lib64:/usr/local/cuda/lib:/opt/amazon/efa/lib64:/usr/local/mpi/lib:/usr/lib64/openmpi/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/mpi/lib:/lib/:\n",
      "2021-04-04 14:20:48.604589: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2.1.0\n",
      "Collecting transformers==2.8.0\n",
      "  Downloading transformers-2.8.0-py3-none-any.whl (563 kB)\n",
      "\u001b[K     |████████████████████████████████| 563 kB 18.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (2.25.1)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (1.19.5)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (3.0.12)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.59.0-py2.py3-none-any.whl (74 kB)\n",
      "\u001b[K     |████████████████████████████████| 74 kB 4.5 MB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (1.17.35)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (2020.11.13)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.44.tar.gz (862 kB)\n",
      "\u001b[K     |████████████████████████████████| 862 kB 133.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers==2.8.0) (0.8)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 114.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers==0.5.2\n",
      "  Downloading tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7 MB 122.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: botocore<1.21.0,>=1.20.35 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->transformers==2.8.0) (1.20.35)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->transformers==2.8.0) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->transformers==2.8.0) (0.3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.21.0,>=1.20.35->boto3->transformers==2.8.0) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.21.0,>=1.20.35->boto3->transformers==2.8.0) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.35->boto3->transformers==2.8.0) (1.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers==2.8.0) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers==2.8.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers==2.8.0) (2.10)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sacremoses->transformers==2.8.0) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sacremoses->transformers==2.8.0) (1.0.1)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.44-py3-none-any.whl size=886084 sha256=c0014cf4f312a9077394cc8e80c8e9e69f6fc9607ab6372a2285cfb233061ca8\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/d4/6d/ad/81106f259084ee9e99156f754f8a4957e4c2cb9c1ccf866f8a\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: tqdm, tokenizers, sentencepiece, sacremoses, transformers\n",
      "Successfully installed sacremoses-0.0.44 sentencepiece-0.1.95 tokenizers-0.5.2 tqdm-4.59.0 transformers-2.8.0\n",
      "Downloading: 100%|███████████████████████████| 232k/232k [00:00<00:00, 57.9MB/s]\n",
      "/opt/ml/config/resourceconfig.json not found.  current_host is unknown.\n",
      "################ START #######################\n",
      "Loaded arguments:\n",
      "Namespace(balance_dataset=False, current_host='unknown', hosts=['unknown'], input_data='data/split', max_seq_length=32, output_data='data/output', test_split_percentage=0.05, train_split_percentage=0.9, validation_split_percentage=0.05)\n",
      "Environment variables:\n",
      "Current host: unknown\n",
      "********** input files ***************\n",
      "args.input_data:  data/split\n",
      "input_files:  ['data/split/tweet_file_02.csv.gz', 'data/split/tweet_file_01.csv.gz']\n",
      "num_cpus 36\n",
      "file data/split/tweet_file_02.csv.gz\n",
      "max_seq_length 32\n",
      "balance_dataset False\n",
      "file data/split/tweet_file_01.csv.gz\n",
      "max_seq_length 32\n",
      "balance_dataset False\n",
      "Shape of dataframe (42385, 2)\n",
      "Shape of dataframe before splitting (42385, 2)\n",
      "train split percentage 0.9\n",
      "validation split percentage 0.05\n",
      "test split percentage 0.05\n",
      "holdout percentage 0.09999999999999998\n",
      "Shape of dataframe (42386, 2)\n",
      "Shape of dataframe before splitting (42386, 2)\n",
      "train split percentage 0.9\n",
      "validation split percentage 0.05\n",
      "test split percentage 0.05\n",
      "holdout percentage 0.09999999999999998\n",
      "test holdout percentage 0.5000000000000001\n",
      "test holdout percentage 0.5000000000000001\n",
      "Shape of train dataframe (38147, 2)\n",
      "Shape of validation dataframe (2119, 2)\n",
      "Shape of test dataframe (2120, 2)\n",
      "Shape of train dataframe (38146, 2)\n",
      "Shape of validation dataframe (2119, 2)\n",
      "Shape of test dataframe (2120, 2)\n",
      "********** Listing tf-record files ***************\n",
      "Listing contents of data/output\n",
      "part-unknown-tweet_file_01.tfrecord\n",
      "part-unknown-tweet_file_02.tfrecord\n",
      "Listing contents of data/output\n",
      "part-unknown-tweet_file_01.tfrecord\n",
      "part-unknown-tweet_file_02.tfrecord\n",
      "Listing contents of data/output\n",
      "part-unknown-tweet_file_01.tfrecord\n",
      "part-unknown-tweet_file_02.tfrecord\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "!python preprocess-scikit-text-to-bert.py \\\n",
    "    --input-data {save_split_data_dir} \\\n",
    "    --output-data {output_data_dir} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 15384\n",
      "drwxrwxr-x 2 ec2-user ec2-user    4096 Apr  4 13:34 .\n",
      "drwxrwxr-x 5 ec2-user ec2-user    4096 Apr  4 13:34 ..\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 7869870 Apr  4 14:21 part-unknown-tweet_file_01.tfrecord\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 7869255 Apr  4 14:21 part-unknown-tweet_file_02.tfrecord\n"
     ]
    }
   ],
   "source": [
    "! ls -al data/output/bert/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Processing Job using Amazon SageMaker\n",
    "멀티 인스턴스(여기서는 2개)로 Preprocessing Job을 실행 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "processor = SKLearnProcessor(framework_version = '0.20.0',\n",
    "                             role = role,\n",
    "                             instance_type = 'ml.c5.2xlarge',\n",
    "                             instance_count = 2\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_raw_input_data = s3_destination_path_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_percentage = 0.90\n",
    "validation_split_percentage = 0.05\n",
    "test_split_percentage = 0.05\n",
    "max_seq_length = 32\n",
    "balance_dataset = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sagemaker-scikit-learn-2021-04-04-14-21-14-399\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-079636235537/tweet_emoticon/csv', 'LocalPath': '/opt/ml/processing/input/data/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'ShardedByS3Key', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-079636235537/sagemaker-scikit-learn-2021-04-04-14-21-14-399/input/code/preprocess-scikit-text-to-bert.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'bert-train', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-079636235537/sagemaker-scikit-learn-2021-04-04-14-21-14-399/output/bert-train', 'LocalPath': '/opt/ml/processing/output/bert/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-validation', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-079636235537/sagemaker-scikit-learn-2021-04-04-14-21-14-399/output/bert-validation', 'LocalPath': '/opt/ml/processing/output/bert/validation', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-test', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-079636235537/sagemaker-scikit-learn-2021-04-04-14-21-14-399/output/bert-test', 'LocalPath': '/opt/ml/processing/output/bert/test', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "processor.run(code='preprocess-scikit-text-to-bert.py',\n",
    "              inputs=[ProcessingInput(source=s3_raw_input_data,\n",
    "                                      destination='/opt/ml/processing/input/data/',\n",
    "                                      s3_data_distribution_type='ShardedByS3Key')\n",
    "              ],\n",
    "              outputs=[\n",
    "                       ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                        output_name='bert-train',\n",
    "                                        source='/opt/ml/processing/output/bert/train'),\n",
    "                       ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                        output_name='bert-validation',\n",
    "                                        source='/opt/ml/processing/output/bert/validation'),\n",
    "                       ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                        output_name='bert-test',\n",
    "                                        source='/opt/ml/processing/output/bert/test'),\n",
    "              ],\n",
    "              arguments=['--train-split-percentage', str(train_split_percentage),\n",
    "                         '--validation-split-percentage', str(validation_split_percentage),\n",
    "                         '--test-split-percentage', str(test_split_percentage),\n",
    "                         '--max-seq-length', str(max_seq_length),\n",
    "                         '--balance-dataset', str(balance_dataset)\n",
    "              ],\n",
    "              logs=True,\n",
    "              wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-scikit-learn-2021-04-04-14-21-14-399\n"
     ]
    }
   ],
   "source": [
    "scikit_processing_job_name = processor.jobs[-1].describe()['ProcessingJobName']\n",
    "print(scikit_processing_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link to CloudWatch \n",
    "CloudWatch에서 생성된 로그를 확인 할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logStream:group=/aws/sagemaker/ProcessingJobs;prefix=sagemaker-scikit-learn-2021-04-04-14-21-14-399;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/ProcessingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>'.format(region, scikit_processing_job_name)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-079636235537/sagemaker-scikit-learn-2021-04-04-14-21-14-399/?region=us-east-1&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>'.format(bucket, scikit_processing_job_name, region)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Status of the Processor Job\n",
    "Processor Job을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "InProgress\n",
      "\n",
      "\n",
      "{'ProcessingInputs': [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-079636235537/tweet_emoticon/csv', 'LocalPath': '/opt/ml/processing/input/data/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'ShardedByS3Key', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-079636235537/sagemaker-scikit-learn-2021-04-04-14-21-14-399/input/code/preprocess-scikit-text-to-bert.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'bert-train', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-079636235537/sagemaker-scikit-learn-2021-04-04-14-21-14-399/output/bert-train', 'LocalPath': '/opt/ml/processing/output/bert/train', 'S3UploadMode': 'EndOfJob'}, 'AppManaged': False}, {'OutputName': 'bert-validation', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-079636235537/sagemaker-scikit-learn-2021-04-04-14-21-14-399/output/bert-validation', 'LocalPath': '/opt/ml/processing/output/bert/validation', 'S3UploadMode': 'EndOfJob'}, 'AppManaged': False}, {'OutputName': 'bert-test', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-079636235537/sagemaker-scikit-learn-2021-04-04-14-21-14-399/output/bert-test', 'LocalPath': '/opt/ml/processing/output/bert/test', 'S3UploadMode': 'EndOfJob'}, 'AppManaged': False}]}, 'ProcessingJobName': 'sagemaker-scikit-learn-2021-04-04-14-21-14-399', 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.c5.2xlarge', 'VolumeSizeInGB': 30}}, 'StoppingCondition': {'MaxRuntimeInSeconds': 86400}, 'AppSpecification': {'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.20.0-cpu-py3', 'ContainerEntrypoint': ['python3', '/opt/ml/processing/input/code/preprocess-scikit-text-to-bert.py'], 'ContainerArguments': ['--train-split-percentage', '0.9', '--validation-split-percentage', '0.05', '--test-split-percentage', '0.05', '--max-seq-length', '32', '--balance-dataset', 'False']}, 'RoleArn': 'arn:aws:iam::079636235537:role/service-role/AmazonSageMaker-ExecutionRole-20210404T215653', 'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:079636235537:processing-job/sagemaker-scikit-learn-2021-04-04-14-21-14-399', 'ProcessingJobStatus': 'InProgress', 'LastModifiedTime': datetime.datetime(2021, 4, 4, 14, 21, 15, 121000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2021, 4, 4, 14, 21, 14, 843000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': 'd159bac5-c738-45f5-b39a-dc6f1fe42dd8', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'd159bac5-c738-45f5-b39a-dc6f1fe42dd8', 'content-type': 'application/x-amz-json-1.1', 'content-length': '2411', 'date': 'Sun, 04 Apr 2021 14:21:15 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "running_processor = sagemaker.processing.ProcessingJob.from_processing_name(processing_job_name=scikit_processing_job_name,\n",
    "                                                                            sagemaker_session=sagemaker_session)\n",
    "\n",
    "processing_job_description = running_processor.describe()\n",
    "\n",
    "processing_job_status = processing_job_description['ProcessingJobStatus']\n",
    "print('\\n')\n",
    "print(processing_job_status)\n",
    "print('\\n')\n",
    "\n",
    "print(processing_job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........................................................!"
     ]
    }
   ],
   "source": [
    "running_processor.wait(logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the Processed Output Data\n",
    "실제 전처리된 데이타를 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-079636235537/sagemaker-scikit-learn-2021-04-04-14-21-14-399/output/bert-train\n",
      "s3://sagemaker-us-east-1-079636235537/sagemaker-scikit-learn-2021-04-04-14-21-14-399/output/bert-validation\n",
      "s3://sagemaker-us-east-1-079636235537/sagemaker-scikit-learn-2021-04-04-14-21-14-399/output/bert-test\n"
     ]
    }
   ],
   "source": [
    "output_config = processing_job_description['ProcessingOutputConfig']\n",
    "for output in output_config['Outputs']:\n",
    "    if output['OutputName'] == 'bert-train':\n",
    "        processed_train_data_s3_uri = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'bert-validation':\n",
    "        processed_validation_data_s3_uri = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'bert-test':\n",
    "        processed_test_data_s3_uri = output['S3Output']['S3Uri']\n",
    "        \n",
    "print(processed_train_data_s3_uri)\n",
    "print(processed_validation_data_s3_uri)\n",
    "print(processed_test_data_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-04 14:25:51    7868946 part-algo-1-tweet_file_01.tfrecord\n",
      "2021-04-04 14:25:51    7869167 part-algo-2-tweet_file_02.tfrecord\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_train_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-04 14:25:52     437330 part-algo-1-tweet_file_01.tfrecord\n",
      "2021-04-04 14:25:52     436790 part-algo-2-tweet_file_02.tfrecord\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_validation_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-04 14:25:52     437690 part-algo-1-tweet_file_01.tfrecord\n",
      "2021-04-04 14:25:52     437571 part-algo-2-tweet_file_02.tfrecord\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_test_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 's3_raw_input_data' (str)\n",
      "Stored 'max_seq_length' (int)\n",
      "Stored 'train_split_percentage' (float)\n",
      "Stored 'validation_split_percentage' (float)\n",
      "Stored 'test_split_percentage' (float)\n",
      "Stored 'processed_train_data_s3_uri' (str)\n",
      "Stored 'processed_validation_data_s3_uri' (str)\n",
      "Stored 'processed_test_data_s3_uri' (str)\n"
     ]
    }
   ],
   "source": [
    "%store s3_raw_input_data\n",
    "%store max_seq_length\n",
    "%store train_split_percentage\n",
    "%store validation_split_percentage\n",
    "%store test_split_percentage\n",
    "%store processed_train_data_s3_uri\n",
    "%store processed_validation_data_s3_uri\n",
    "%store processed_test_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-079636235537/sagemaker-scikit-learn-2021-04-04-14-21-14-399/output/bert-train\n",
      "s3://sagemaker-us-east-1-079636235537/sagemaker-scikit-learn-2021-04-04-14-21-14-399/output/bert-validation\n",
      "s3://sagemaker-us-east-1-079636235537/sagemaker-scikit-learn-2021-04-04-14-21-14-399/output/bert-test\n"
     ]
    }
   ],
   "source": [
    "print(processed_train_data_s3_uri)\n",
    "print(processed_validation_data_s3_uri)\n",
    "print(processed_test_data_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
