{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Module 4.1.2] inference on model from Kubeflow\n",
    "\n",
    "\n",
    "---\n",
    "μ΄ λ…ΈνΈλ¶μ€ μ•½ 10μ •λ„ μ‹κ°„μ΄ μ†μ” λ©λ‹λ‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert2tweet-2020-08-02-12-25-01-260\n"
     ]
    }
   ],
   "source": [
    "# training_job_name = 'TrainingJob-20200802135153-A952'\n",
    "print(training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference_image:  343441690612.dkr.ecr.ap-northeast-2.amazonaws.com/sagemaker-tensorflow-serving:2.0.0-cpu\n"
     ]
    }
   ],
   "source": [
    "inference_image = '343441690612.dkr.ecr.ap-northeast-2.amazonaws.com/sagemaker-tensorflow-serving:2.0.0-cpu'\n",
    "# inference_image = ecr_infer_custom_image_tf_serving_20_cpu\n",
    "print(\"inference_image: \", inference_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = 's3://sagemaker-ap-northeast-2-343441690612/sagemaker-scikit-learn-2020-08-02-01-14-52-546/model/TrainingJob-20200802135153-A952/output/model.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation with Custome TFS Docker Image and Inference code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.serving import Model\n",
    "\n",
    "model = Model(model_data= model_data,\n",
    "              role=role,\n",
    "              entry_point='inference.py',\n",
    "              image = inference_image\n",
    "             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-ap-northeast-2-343441690612/sagemaker-scikit-learn-2020-08-02-01-14-52-546/model/TrainingJob-20200802135153-A952/output/model.tar.gz'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.image\n",
    "model.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## μ—”λ“ν¬μΈνΈ μƒμ„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to tmp57hgdwur_algo-1-ukhlw_1\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m INFO:__main__:starting services\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m INFO:__main__:nginx config: \n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m load_module modules/ngx_http_js_module.so;\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m worker_processes auto;\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m daemon off;\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m pid /tmp/nginx.pid;\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m error_log  /dev/stderr error;\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m worker_rlimit_nofile 4096;\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m events {\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m   worker_connections 2048;\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m }\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m http {\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m   include /etc/nginx/mime.types;\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m   default_type application/json;\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m   access_log /dev/stdout combined;\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m   js_include tensorflow-serving.js;\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m   upstream tfs_upstream {\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m     server localhost:8501;\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m   }\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m   upstream gunicorn_upstream {\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m     server unix:/tmp/gunicorn.sock fail_timeout=1;\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m   }\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m   server {\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m     listen 8080 deferred;\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m     client_max_body_size 0;\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m     client_body_buffer_size 100m;\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m     subrequest_output_buffer_size 100m;\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m     set $tfs_version 2.0;\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m     set $default_tfs_model None;\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m     location /tfs {\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m         rewrite ^/tfs/(.*) /$1  break;\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m         proxy_redirect off;\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m         proxy_pass_request_headers off;\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m         proxy_set_header Content-Type 'application/json';\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m         proxy_set_header Accept 'application/json';\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m         proxy_pass http://tfs_upstream;\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m     }\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m     location /ping {\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m         proxy_pass http://gunicorn_upstream/ping;\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m     }\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m     location /invocations {\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m         proxy_pass http://gunicorn_upstream/invocations;\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m     }\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m     location /models {\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m         proxy_pass http://gunicorn_upstream/models;\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m     }\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m     location / {\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m         return 404 '{\"error\": \"Not Found\"}';\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m     }\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m     keepalive_timeout 3;\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m   }\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m }\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m INFO:tfs_utils:using default model name: saved_model\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m INFO:tfs_utils:tensorflow serving model config: \n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m model_config_list: {\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m   config: {\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m     name: \"saved_model\",\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m     base_path: \"/opt/ml/model/tensorflow/saved_model\",\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m     model_platform: \"tensorflow\"\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m   }\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m }\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m INFO:__main__:using default model name: saved_model\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m INFO:__main__:tensorflow serving model config: \n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m model_config_list: {\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m   config: {\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m     name: \"saved_model\",\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m     base_path: \"/opt/ml/model/tensorflow/saved_model\",\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m     model_platform: \"tensorflow\"\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m   }\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m }\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m \n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m INFO:__main__:tensorflow version info:\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m TensorFlow ModelServer: 2.0.0+dev.sha.642edcd\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m TensorFlow Library: 2.0.0\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m INFO:__main__:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=/sagemaker/model-config.cfg --max_num_load_retries=0 \n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m INFO:__main__:started tensorflow serving (pid: 8)\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m INFO:__main__:gunicorn command: gunicorn -b unix:/tmp/gunicorn.sock -k gevent --chdir /sagemaker --pythonpath /opt/ml/model/code -e TFS_GRPC_PORT=9000 -e SAGEMAKER_MULTI_MODEL=False -e SAGEMAKER_SAFE_PORT_RANGE=None python_service:app\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m 2020-08-02 14:32:34.832243: I tensorflow_serving/model_servers/server_core.cc:462] Adding/updating models.\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m 2020-08-02 14:32:34.832270: I tensorflow_serving/model_servers/server_core.cc:573]  (Re-)adding model: saved_model\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m 2020-08-02 14:32:34.932590: I tensorflow_serving/util/retrier.cc:46] Retrying of Reserving resources for servable: {name: saved_model version: 0} exhausted max_num_retries: 0\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m 2020-08-02 14:32:34.932630: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: saved_model version: 0}\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m 2020-08-02 14:32:34.932642: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: saved_model version: 0}\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m 2020-08-02 14:32:34.932671: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: saved_model version: 0}\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m 2020-08-02 14:32:34.932698: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /opt/ml/model/tensorflow/saved_model/0\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m 2020-08-02 14:32:34.999726: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m INFO:__main__:gunicorn version info:\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m gunicorn (version 20.0.4)\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m INFO:__main__:started gunicorn (pid: 44)\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m 2020-08-02 14:32:35.076519: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX512F\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m 2020-08-02 14:32:35.088761: I external/org_tensorflow/tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m [2020-08-02 14:32:35 +0000] [44] [INFO] Starting gunicorn 20.0.4\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m [2020-08-02 14:32:35 +0000] [44] [INFO] Listening at: unix:/tmp/gunicorn.sock (44)\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m INFO:__main__:gunicorn server is ready!\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m [2020-08-02 14:32:35 +0000] [44] [INFO] Using worker: gevent\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m [2020-08-02 14:32:35 +0000] [59] [INFO] Booting worker with pid: 59\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m INFO:__main__:nginx version info:\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m nginx version: nginx/1.18.0\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m built by gcc 7.4.0 (Ubuntu 7.4.0-1ubuntu1~18.04.1) \n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m built with OpenSSL 1.1.1  11 Sep 2018\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m TLS SNI support enabled\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m configure arguments: --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt='-g -O2 -fdebug-prefix-map=/data/builder/debuild/nginx-1.18.0/debian/debuild-base/nginx-1.18.0=. -fstack-protector-strong -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fPIC' --with-ld-opt='-Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-z,now -Wl,--as-needed -pie'\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m INFO:__main__:started nginx (pid: 60)\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m 2020-08-02 14:32:35.250884: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:202] Restoring SavedModel bundle.\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m 2020-08-02 14:32:36.584163: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:151] Running initialization op on SavedModel bundle at path: /opt/ml/model/tensorflow/saved_model/0\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m 2020-08-02 14:32:36.801460: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:311] SavedModel load for tags { serve }; Status: success. Took 1868761 microseconds.\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m 2020-08-02 14:32:36.815924: I tensorflow_serving/servables/tensorflow/saved_model_warmup.cc:105] No warmup data file found at /opt/ml/model/tensorflow/saved_model/0/assets.extra/tf_serving_warmup_requests\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m 2020-08-02 14:32:36.832327: I tensorflow_serving/util/retrier.cc:46] Retrying of Loading servable: {name: saved_model version: 0} exhausted max_num_retries: 0\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m 2020-08-02 14:32:36.832357: I tensorflow_serving/core/loader_harness.cc:87] Successfully loaded servable version {name: saved_model version: 0}\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m 2020-08-02 14:32:36.836375: I tensorflow_serving/model_servers/server.cc:353] Running gRPC ModelServer at 0.0.0.0:9000 ...\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m [warn] getaddrinfo: address family for nodename not supported\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m 2020-08-02 14:32:36.837686: I tensorflow_serving/model_servers/server.cc:373] Exporting HTTP/REST API at:localhost:8501 ...\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m [evhttp_server.cc : 238] NET_LOG: Entering the event loop ...\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m 2020-08-02 14:32:36.981539: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:$LD_LIBRARY_PATH\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m 2020-08-02 14:32:36.981649: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:$LD_LIBRARY_PATH\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m 2020-08-02 14:32:36.981663: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m INFO:transformers.file_utils:TensorFlow version 2.1.0 available.\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m INFO:filelock:Lock 140277351689968 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m INFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpyqyq30ac\n",
      "Downloading: 100% 232k/232k [00:00<00:00, 396kB/s]  \n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m INFO:transformers.file_utils:storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m INFO:transformers.file_utils:creating metadata file for /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m INFO:filelock:Lock 140277351689968 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "\u001b[36malgo-1-ukhlw_1  |\u001b[0m INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "!\u001b[36malgo-1-ukhlw_1  |\u001b[0m 172.19.0.1 - - [02/Aug/2020:14:32:41 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"-\"\n",
      "CPU times: user 1min 12s, sys: 14.5 s, total: 1min 27s\n",
      "Wall time: 1min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# instance_type='ml.m4.xlarge'\n",
    "instance_type='local'\n",
    "deployed_model = model.deploy(initial_instance_count = 1,\n",
    "                             instance_type = instance_type,\n",
    "                             wait=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor Creation on the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-tensorflow-serving-2020-08-02-13-05-44-541\n"
     ]
    }
   ],
   "source": [
    "# tweet_bert_endpoint_name = 'train_text, train_label, test_text, test_label = tweet_data.split_train_test_data(texts, labels, 0.9)\n",
    "tweet_bert_endpoint_name = deployed_model.endpoint\n",
    "print(tweet_bert_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.tensorflow.serving import Predictor\n",
    "\n",
    "predictor = Predictor(endpoint_name = tweet_bert_endpoint_name,\n",
    "                      sagemaker_session = sess,\n",
    "                      content_type = 'application/json',\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference μ‹¤ν–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emoji_to_idx is loaded\n",
      "π‚\n"
     ]
    }
   ],
   "source": [
    "from TweetUtil import TweetUtil\n",
    "\n",
    "tweet_util = TweetUtil()\n",
    "tweet_util.load_emoji_data('emoji_to_idx.pickle')\n",
    "emoji = tweet_util.get_emo_class_label(3)\n",
    "print(emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TWEET</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>my bestf and fav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7897</th>\n",
       "      <td>this type of haircut makes me feel some type ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4267</th>\n",
       "      <td>tbt warning don't ever try that</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8451</th>\n",
       "      <td>thanks tito aldubmaghihintay</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2958</th>\n",
       "      <td>i am so sad</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>i get so annoyed when shit don't go right</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4191</th>\n",
       "      <td>so much truth goodnight</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260</th>\n",
       "      <td>a likely story</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6157</th>\n",
       "      <td>good points need to do more research</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9018</th>\n",
       "      <td>it doesnt get hotter than u nicole</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  TWEET  LABEL\n",
       "2263                                   my bestf and fav      0\n",
       "7897   this type of haircut makes me feel some type ...      5\n",
       "4267                    tbt warning don't ever try that      4\n",
       "8451                       thanks tito aldubmaghihintay      4\n",
       "2958                                        i am so sad      6\n",
       "1342          i get so annoyed when shit don't go right      8\n",
       "4191                           so much truth goodnight       0\n",
       "2260                                    a likely story       9\n",
       "6157              good points need to do more research       9\n",
       "9018                 it doesnt get hotter than u nicole      2"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file_path = 'data/test/tweet_file_test.csv'\n",
    "test_df = pd.read_csv(test_file_path)\n",
    "test_file_path = 'data/test/tweet_file_test.csv'\n",
    "test_df = pd.read_csv(test_file_path)\n",
    "sample_df = test_df.sample(10)\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top_N_label(score_list, topN):\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    top_n_idx = np.argsort(score_list)[-topN:]\n",
    "    top_n_values = [score_list[i] for i in top_n_idx]\n",
    "    \n",
    "    top_n_idx_list = top_n_idx.tolist()\n",
    "    top_n_idx_list.reverse()\n",
    "    top_n_values = [score_list[i] for i in top_n_idx_list]    \n",
    "    \n",
    "    return top_n_idx_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 3 μ΄λ¨ν‹°μ½ μ¶”μ²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews: \n",
      " ['my bestf and fav']\n",
      "tweet: my bestf and fav \n",
      "Ground_truth- 0:β¤\n",
      " \n",
      "Prediction: 1,π’•,5,π,4,π,0,β¤,2,π”¥ \n",
      " \n",
      "reviews: \n",
      " [\" this type of haircut makes me feel some type of way i love basti's hair timyfightforlove\"]\n",
      "tweet:  this type of haircut makes me feel some type of way i love basti's hair timyfightforlove \n",
      "Ground_truth- 5:π\n",
      " \n",
      "Prediction: 5,π,0,β¤,7,π­,3,π‚,1,π’• \n",
      " \n",
      "reviews: \n",
      " [\" tbt warning don't ever try that\"]\n",
      "tweet:  tbt warning don't ever try that \n",
      "Ground_truth- 4:π\n",
      " \n",
      "Prediction: 3,π‚,8,π™„,7,π­,9,π¤”,0,β¤ \n",
      " \n",
      "reviews: \n",
      " [' thanks tito aldubmaghihintay']\n",
      "tweet:  thanks tito aldubmaghihintay \n",
      "Ground_truth- 4:π\n",
      " \n",
      "Prediction: 4,π,1,π’•,2,π”¥,0,β¤,5,π \n",
      " \n",
      "reviews: \n",
      " ['i am so sad']\n",
      "tweet: i am so sad \n",
      "Ground_truth- 6:π©\n",
      " \n",
      "Prediction: 0,β¤,6,π©,7,π­,8,π™„,4,π \n",
      " \n",
      "reviews: \n",
      " [\"i get so annoyed when shit don't go right\"]\n",
      "tweet: i get so annoyed when shit don't go right \n",
      "Ground_truth- 8:π™„\n",
      " \n",
      "Prediction: 8,π™„,3,π‚,9,π¤”,6,π©,7,π­ \n",
      " \n",
      "reviews: \n",
      " ['so much truth goodnight ']\n",
      "tweet: so much truth goodnight  \n",
      "Ground_truth- 0:β¤\n",
      " \n",
      "Prediction: 5,π,1,π’•,0,β¤,4,π,2,π”¥ \n",
      " \n",
      "reviews: \n",
      " [' a likely story ']\n",
      "tweet:  a likely story  \n",
      "Ground_truth- 9:π¤”\n",
      " \n",
      "Prediction: 7,π­,6,π©,0,β¤,3,π‚,8,π™„ \n",
      " \n",
      "reviews: \n",
      " [' good points need to do more research ']\n",
      "tweet:  good points need to do more research  \n",
      "Ground_truth- 9:π¤”\n",
      " \n",
      "Prediction: 4,π,6,π©,0,β¤,7,π­,1,π’• \n",
      " \n",
      "reviews: \n",
      " [' it doesnt get hotter than u nicole']\n",
      "tweet:  it doesnt get hotter than u nicole \n",
      "Ground_truth- 2:π”¥\n",
      " \n",
      "Prediction: 6,π©,3,π‚,7,π­,8,π™„,2,π”¥ \n",
      " \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "\n",
    "columns = ['TWEET', 'LABEL']\n",
    "topN = 5\n",
    "for tweet, label in zip(sample_df.TWEET.values, sample_df.LABEL.values):\n",
    "    # print(\"label: {}, tweet: {}\".format(label, tweet))\n",
    "    \n",
    "    reviews = [tweet]\n",
    "    \n",
    "    print(\"reviews: \\n\", reviews)\n",
    "\n",
    "\n",
    "\n",
    "    predicted_classes = predictor.predict(reviews)[0]\n",
    "    predicted_classes = show_top_N_label(predicted_classes, topN)\n",
    "\n",
    "    print('tweet: {} \\nGround_truth- {}:{}\\n '.format(\n",
    "        tweet,\n",
    "        label, \n",
    "        tweet_util.get_emo_class_label(label))\n",
    "         )    \n",
    "    \n",
    "\n",
    "    print('Prediction: {},{},{},{},{},{},{},{},{},{} \\n '.format(\n",
    "        predicted_classes[0], \n",
    "        tweet_util.get_emo_class_label(predicted_classes[0]),\n",
    "        predicted_classes[1], \n",
    "        tweet_util.get_emo_class_label(predicted_classes[1]),\n",
    "        predicted_classes[2], \n",
    "        tweet_util.get_emo_class_label(predicted_classes[2]),   \n",
    "        predicted_classes[3], \n",
    "        tweet_util.get_emo_class_label(predicted_classes[3]),                                      \n",
    "        predicted_classes[4], \n",
    "        tweet_util.get_emo_class_label(predicted_classes[4]),                                      \n",
    "        \n",
    "        ))    \n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
